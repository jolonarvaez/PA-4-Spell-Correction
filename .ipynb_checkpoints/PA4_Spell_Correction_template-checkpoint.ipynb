{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i3m9JjeM5U5"
   },
   "source": [
    "# **Programming Assessment \\#4**\n",
    "\n",
    "Names: Enriquez, Manolo L.  Narvaez, Jose Wilfredo S.\n",
    "\n",
    "More information on the assessment is found in our Canvas course. Link: https://dlsu.instructure.com/courses/93383/assignments/739602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxtmCAZwNoeU"
   },
   "source": [
    "# **Load Data**\n",
    "\n",
    "*While you don't have to separate your code into blocks, it might be easier if you separated loading your data from actually implementation of your code. Consider placing all loading of data into the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CbvxU2oTM4IV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\jolon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Language Model\n",
    "all_words = Counter()\n",
    "for filename in nltk.corpus.gutenberg.fileids():\n",
    "  words = [word.lower() for word in nltk.corpus.gutenberg.words(filename)]\n",
    "  all_words.update(words)\n",
    "\n",
    "total_tokens = sum(all_words.values())\n",
    "total_words = len(all_words)\n",
    "\n",
    "# Error Model class\n",
    "class ErrorModel:\n",
    "    def __init__(self, character, correction, occurence):\n",
    "        self.character  = character\n",
    "        self.correction = correction\n",
    "        self.occurence = occurence\n",
    "\n",
    "# Loading error model text file into dictionary\n",
    "error_model = {}\n",
    "file1 = open('count_1edit.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    "for line in Lines:\n",
    "    key = line.split('\\t')[0]\n",
    "    value = int(line.split('\\t')[1])\n",
    "    error_model[key] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8YCZLi-N0uR"
   },
   "source": [
    "# **Noisy Channel Model Implementation**\n",
    "\n",
    "*Again, you don't have to follow this directly, but consider placing your implementation of the model in the code block below. And while we discussed the general approach in class, kindly describe how you decided to implement the spell correction model. Include any modifications your group made as well. This might be a good spot to place part of your write up.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "VqKjpUrkOSnC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: acress\n",
      "across 0.00559767590165096 [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6)]\n",
      "acres 0.0001417133139658471 [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 5)]\n",
      "access 0.0002834266279316942 [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6)]\n",
      "cress 2.3618885660974514e-05 [(0, 0), (1, 1), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5)]\n",
      "caress 0.0001653321996268216 [(0, 0), (1, 1), (1, 2), (2, 3), (3, 3), (4, 4), (5, 5), (6, 6)]\n",
      "actress 0.00021256997094877065 [(0, 0), (1, 1), (2, 2), (3, 3), (3, 4), (4, 5), (5, 6), (6, 7)]\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Input: \")\n",
    "\n",
    "one_edit_list = []\n",
    "\n",
    "if word not in all_words:\n",
    "    # Look for words with one edit distance in language model\n",
    "    for key in all_words:\n",
    "        edit_distance = nltk.edit_distance(word, key, substitution_cost=1, transpositions=True)\n",
    "        if edit_distance == 1:\n",
    "            one_edit_list.append(key)\n",
    "            \n",
    "    for candidate in one_edit_list:\n",
    "        p_word = all_words[candidate] / total_words\n",
    "        print(candidate, end=\" \")\n",
    "        print(p_word, end=\" \")\n",
    "        backtrace = nltk.edit_distance_align(word, candidate, substitution_cost=1)\n",
    "        print(backtrace)\n",
    "else:\n",
    "    print(\"Output: No error\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a c\n"
     ]
    }
   ],
   "source": [
    "s1 = \"acress\"\n",
    "s2 = \"caress\"\n",
    "test = [(0, 0), (1, 1), (2, 2), (3, 3), (3, 4), (4, 5), (5, 6), (6, 7)]\n",
    "\n",
    "# check for alignments to get the correct and error letters\n",
    "for i in range(len(test)):\n",
    "    correct_letter = ''\n",
    "    error_letter = ''\n",
    "    pair = test[i]\n",
    "    s1_idx = pair[0]\n",
    "    s2_idx = pair[1]\n",
    "    \n",
    "    print(s1[s1_idx] + \" \" + s2[s2_idx])\n",
    "    \n",
    "    if s1[s1_idx] != s2[s2_idx]:\n",
    "        \n",
    "        if s1[s1_idx+1] == s2[s2_idx] and s1[s1_idx] == s2[s2_idx+1]:\n",
    "            error_letter = s1[s1_idx+1] + s2[s2_idx]\n",
    "            correct_letter = s1[s1_idx] + s2[s2_idx+1]\n",
    "            break\n",
    "            \n",
    "        # check for substitution\n",
    "        elif s1[s1_idx+1] == s2[s2_idx+1]:\n",
    "            print('substitution')\n",
    "            break\n",
    "            \n",
    "        # check for insertion\n",
    "        if s1[s1_idx] == s2[s2_idx+1]:\n",
    "            print('insertion')\n",
    "            break\n",
    "        \n",
    "        # check for deletion\n",
    "        if s1[s1_idx+1] == s2[s2_idx]:\n",
    "            print('deletion')\n",
    "            break\n",
    "            \n",
    "        # check for transposition\n",
    "        \n",
    "            \n",
    "    print(error_letter + \" \" + correct_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3smvUR6OXUa"
   },
   "source": [
    "# **Your Relfection / Takeaway / Analysis**\n",
    "\n",
    "*Kindly place the rest of your write up. More information is found in the Canvas write up.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PA4_Spell_Correction_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
