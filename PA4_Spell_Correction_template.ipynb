{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i3m9JjeM5U5"
   },
   "source": [
    "# **Programming Assessment \\#4**\n",
    "\n",
    "Names: Enriquez, Manolo L.  Narvaez, Jose Wilfredo S.\n",
    "\n",
    "More information on the assessment is found in our Canvas course. Link: https://dlsu.instructure.com/courses/93383/assignments/739602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxtmCAZwNoeU"
   },
   "source": [
    "# **Load Data**\n",
    "\n",
    "*While you don't have to separate your code into blocks, it might be easier if you separated loading your data from actually implementation of your code. Consider placing all loading of data into the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbvxU2oTM4IV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Language Model\n",
    "all_words = Counter()\n",
    "for filename in nltk.corpus.gutenberg.fileids():\n",
    "    words = [word.lower() for word in nltk.corpus.gutenberg.words(filename)]\n",
    "    all_words.update(words)\n",
    "\n",
    "total_tokens = sum(all_words.values())\n",
    "total_words = len(all_words)\n",
    "\n",
    "# Loading error model text file into dictionary\n",
    "error_model = {}\n",
    "file1 = open('count_1edit.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    "for line in Lines:\n",
    "    key = line.split('\\t')[0]\n",
    "    value = int(line.split('\\t')[1])\n",
    "    error_model[key] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8YCZLi-N0uR"
   },
   "source": [
    "# **Noisy Channel Model Implementation**\n",
    "\n",
    "*Again, you don't have to follow this directly, but consider placing your implementation of the model in the code block below. And while we discussed the general approach in class, kindly describe how you decided to implement the spell correction model. Include any modifications your group made as well. This might be a good spot to place part of your write up.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqKjpUrkOSnC"
   },
   "outputs": [],
   "source": [
    "word = input(\"Input: \")\n",
    "\n",
    "one_edit_list = []\n",
    "output = {}\n",
    "\n",
    "if word not in all_words:\n",
    "    # Look for words with one edit distance in language model\n",
    "    for key in all_words:\n",
    "        edit_distance = nltk.edit_distance(word, key, substitution_cost=1, transpositions=True)\n",
    "        if edit_distance == 1:\n",
    "            one_edit_list.append(key)\n",
    "     \n",
    "    # iterate through each word with edit distance of 1\n",
    "    for candidate in one_edit_list:\n",
    "        \n",
    "        # calculate p(word) by getting word occurence count divided \n",
    "        # by total number of words in corpus\n",
    "        p_word = all_words[candidate] / total_words\n",
    "        \n",
    "        # getting backtrace for edit distance alignment\n",
    "        backtrace = nltk.edit_distance_align(word, candidate, substitution_cost=1)\n",
    "        \n",
    "        s1 = word\n",
    "        s2 = candidate\n",
    "        prev_idx_1 = ''\n",
    "        prev_idx_2 = ''\n",
    "        \n",
    "        # traverse tuple in reverse order to detect the errors\n",
    "        for pair in reversed(backtrace):\n",
    "            error_letter = ''\n",
    "            correct_letter = ''\n",
    "            error =''\n",
    "            s1_idx = pair[0]-1\n",
    "            s2_idx = pair[1]-1\n",
    "            \n",
    "            if s1_idx >= 0  and s2_idx >= 0:\n",
    "                \n",
    "                \n",
    "                if s1_idx == s2_idx:\n",
    "                    \n",
    "                    # if the current tuples are equal, check if the previous tuple pair if they are different\n",
    "                    # checks for deletion\n",
    "                    if prev_idx_1 != s1_idx and prev_idx_2 == s2_idx:\n",
    "                        error_letter = s1[s1_idx] + s1[s1_idx+1]\n",
    "                        correct_letter = s2[s2_idx]\n",
    "                        break\n",
    "                        \n",
    "                    # checks for insertion\n",
    "                    elif prev_idx_1 == s1_idx and prev_idx_2 != s2_idx:\n",
    "                        error_letter = s1[s1_idx-1]\n",
    "                        correct_letter = s2[s2_idx-1] + s2[s2_idx] \n",
    "                        break\n",
    "                        \n",
    "                    # if the tuple is equal but index character of input and candidate is different    \n",
    "                    # checks for substitution\n",
    "                    elif s1[s1_idx] != s2[s2_idx]:\n",
    "                        error_letter = s1[s1_idx]\n",
    "                        correct_letter = s2[s2_idx]\n",
    "                        break \n",
    "\n",
    "                prev_idx_1 = s1_idx \n",
    "                prev_idx_2 = s2_idx \n",
    "             \n",
    "        # create x|w / error\n",
    "        error = error_letter + '|' + correct_letter\n",
    "        \n",
    "        # if the error exists in the error model, calculate noisy channel probability\n",
    "        if error in error_model:\n",
    "            ctr = 0\n",
    "            for words in all_words:\n",
    "                if correct_letter in words:\n",
    "                    ctr += 1\n",
    "                    \n",
    "            # *P(x|w)P(w) \n",
    "            output[candidate] = (error_model[error] / ctr) * p_word\n",
    "            \n",
    "    # sorting output dictionary by the value\n",
    "    print('Output: ', end =\"\")\n",
    "    output_list =  sorted(output.items(), key=lambda x:x[1])\n",
    "\n",
    "    # prints out all the candidates for spelling checking and their probability \n",
    "    for i in reversed(output_list):\n",
    "        print(i[0] + \" (\" + str(i[1]) + \")\", end = \", \")\n",
    "else:\n",
    "    print(\"Output: No error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3smvUR6OXUa"
   },
   "source": [
    "# **Your Relfection / Takeaway / Analysis**\n",
    "\n",
    "*Kindly place the rest of your write up. More information is found in the Canvas write up.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PA4_Spell_Correction_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
